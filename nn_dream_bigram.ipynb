{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt # data set containing all of shakesphere's work as a text file"
      ],
      "metadata": {
        "id": "fNMUsDLVGCkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b823af23-d5e7-4203-dc56-e6b6685f91be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-21 10:56:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.5’\n",
            "\n",
            "\rinput.txt.5           0%[                    ]       0  --.-KB/s               \rinput.txt.5         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-08-21 10:56:39 (17.1 MB/s) - ‘input.txt.5’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"input.txt\", 'r')\n",
        "text = file.read()\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD6mbWzJTYeQ",
        "outputId": "ad40f153-5e7c-4ff2-e949-eeb66315a9f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt71fU2-VIoA",
        "outputId": "9e496bf4-fbb9-48c7-cdcc-a7221eef0039"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the most basic character level tokenizer, because our tokenizer is the most basic the context length will be big compared to if we used sub word level tokenizer.\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # Output: {'a': 0, 'b': 1, 'c': 2}\n",
        "itos = { i:ch for i,ch in enumerate(chars) } #\n",
        "def encode(s):\n",
        "  return [stoi[c] for c in s]\n",
        "def decode(l):\n",
        "  return ''.join([itos[c] for c in l])\n",
        "\n",
        "print(decode(encode(\"I am shakesphere!\")))\n",
        "print(\"encoded vector length:\", len(encode(\"I am shakesphere!\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bXkFq63Vj48",
        "outputId": "fa011429-4831-402f-bb16-45851711de9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am shakesphere!\n",
            "encoded vector length: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = torch.tensor(encode(text))\n",
        "data.shape, data.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-j0i0kCV-CV",
        "outputId": "3522e955-4a41-4832-b944-00b23acdc30c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1115394]), torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(data))\n",
        "val_size = int(0.1 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "train_data.shape, val_data.shape, test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoQLiqrLanPE",
        "outputId": "0bf9a308-6626-460a-a503-4ab1f3548083"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([892315]), torch.Size([111539]), torch.Size([111540]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 16\n",
        "block_size = 32 # maximum context length for prediction\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:', xb)\n",
        "print('targets:', yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI6Poa-qaw35",
        "outputId": "d4e61d05-8ec1-41f4-9729-920f64f71495"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: tensor([[39, 56, 58, 46,  8,  0, 13, 50, 50,  1, 51, 39, 63,  1, 40, 43,  1, 61,\n",
            "         43, 50, 50, 11,  1, 40, 59, 58,  6,  1, 47, 44,  1, 19],\n",
            "        [43,  8,  0, 32, 46, 43, 56, 43,  5, 57,  1, 57, 53, 51, 43,  1, 39, 51,\n",
            "         53, 52, 45,  1, 63, 53, 59,  1, 46, 39, 60, 43,  1, 40],\n",
            "        [50, 53,  6,  0, 21,  1, 41, 53, 52, 48, 59, 56, 43,  1, 58, 46, 43, 43,\n",
            "          6,  1, 40, 63,  1, 39, 50, 50,  1, 58, 46, 43,  1, 54],\n",
            "        [ 1, 45, 53, 53, 42,  1, 51, 63,  1, 50, 53, 56, 42, 11,  0, 18, 53, 56,\n",
            "          1, 53, 52,  1, 58, 46, 39, 58,  1, 45, 56, 53, 59, 52],\n",
            "        [56, 47, 43, 52, 42,  8,  0,  0, 32, 46, 47, 56, 42,  1, 35, 39, 58, 41,\n",
            "         46, 51, 39, 52, 10,  0, 27,  6,  1, 47, 57,  1, 47, 58],\n",
            "        [53, 59, 58, 57,  0, 20, 53, 61,  1, 63, 53, 59,  1, 41, 39, 52,  1, 44,\n",
            "         56, 53, 61, 52,  1, 58, 46, 39, 52,  1, 57, 54, 43, 52],\n",
            "        [43, 58, 58, 50, 43, 42,  1, 54, 56, 53, 48, 43, 41, 58,  0, 25, 39, 63,\n",
            "          1, 57, 59, 44, 44, 43, 56,  1, 39, 50, 58, 43, 56, 39],\n",
            "        [57,  1, 50, 47, 49, 43,  1, 58, 46, 53, 56, 52,  8,  0,  0, 25, 17, 30,\n",
            "         15, 33, 32, 21, 27, 10,  0, 21, 44,  1, 50, 53, 60, 43],\n",
            "        [56, 53, 53, 58,  1, 53, 44,  1, 39, 52, 41, 47, 43, 52, 58,  1, 43, 52,\n",
            "         60, 63,  8,  1, 21, 44,  1, 22, 59, 54, 47, 58, 43, 56],\n",
            "        [ 1, 39, 52, 42,  1, 58, 53, 54,  1, 53, 44,  1, 54, 56, 39, 47, 57, 43,\n",
            "         57,  1, 60, 53, 59, 41, 46,  5, 42,  6,  0, 35, 53, 59],\n",
            "        [ 6,  0, 32, 46, 39, 58,  1, 58, 46, 47, 57,  1, 57, 39, 51, 43,  1, 60,\n",
            "         43, 56, 63,  1, 42, 39, 63,  1, 63, 53, 59, 56,  1, 43],\n",
            "        [ 1, 61, 39, 63,  8,  0,  0, 34, 27, 24, 33, 25, 26, 21, 13, 10,  0, 27,\n",
            "          6,  1, 63, 43,  5, 56, 43,  1, 61, 43, 50, 50,  1, 51],\n",
            "        [ 1, 47, 52, 42, 43, 43, 42,  6,  0, 25, 53, 56, 43,  1, 41, 56, 47, 51,\n",
            "         47, 52, 39, 50,  1, 47, 52,  1, 58, 46, 43, 43,  1, 58],\n",
            "        [12,  0, 13, 52, 42,  1, 57, 58, 43, 43, 54,  5, 42,  1, 47, 52,  1, 40,\n",
            "         50, 53, 53, 42, 12,  1, 13, 46,  6,  1, 61, 46, 39, 58],\n",
            "        [52, 43,  1, 59, 52, 58, 53,  1, 51, 63, 57, 43, 50, 44, 12,  0, 27,  6,\n",
            "          1, 52, 53,  2,  1, 39, 50, 39, 57,  6,  1, 21,  1, 56],\n",
            "        [33, 15, 21, 27, 10,  0, 13, 63,  6,  1, 61, 46, 63,  1, 52, 53, 58, 12,\n",
            "          1, 19, 56, 39, 41, 43,  1, 47, 57,  1, 45, 56, 39, 41]])\n",
            "targets: tensor([[56, 58, 46,  8,  0, 13, 50, 50,  1, 51, 39, 63,  1, 40, 43,  1, 61, 43,\n",
            "         50, 50, 11,  1, 40, 59, 58,  6,  1, 47, 44,  1, 19, 53],\n",
            "        [ 8,  0, 32, 46, 43, 56, 43,  5, 57,  1, 57, 53, 51, 43,  1, 39, 51, 53,\n",
            "         52, 45,  1, 63, 53, 59,  1, 46, 39, 60, 43,  1, 40, 43],\n",
            "        [53,  6,  0, 21,  1, 41, 53, 52, 48, 59, 56, 43,  1, 58, 46, 43, 43,  6,\n",
            "          1, 40, 63,  1, 39, 50, 50,  1, 58, 46, 43,  1, 54, 39],\n",
            "        [45, 53, 53, 42,  1, 51, 63,  1, 50, 53, 56, 42, 11,  0, 18, 53, 56,  1,\n",
            "         53, 52,  1, 58, 46, 39, 58,  1, 45, 56, 53, 59, 52, 42],\n",
            "        [47, 43, 52, 42,  8,  0,  0, 32, 46, 47, 56, 42,  1, 35, 39, 58, 41, 46,\n",
            "         51, 39, 52, 10,  0, 27,  6,  1, 47, 57,  1, 47, 58,  1],\n",
            "        [59, 58, 57,  0, 20, 53, 61,  1, 63, 53, 59,  1, 41, 39, 52,  1, 44, 56,\n",
            "         53, 61, 52,  1, 58, 46, 39, 52,  1, 57, 54, 43, 52, 42],\n",
            "        [58, 58, 50, 43, 42,  1, 54, 56, 53, 48, 43, 41, 58,  0, 25, 39, 63,  1,\n",
            "         57, 59, 44, 44, 43, 56,  1, 39, 50, 58, 43, 56, 39, 58],\n",
            "        [ 1, 50, 47, 49, 43,  1, 58, 46, 53, 56, 52,  8,  0,  0, 25, 17, 30, 15,\n",
            "         33, 32, 21, 27, 10,  0, 21, 44,  1, 50, 53, 60, 43,  1],\n",
            "        [53, 53, 58,  1, 53, 44,  1, 39, 52, 41, 47, 43, 52, 58,  1, 43, 52, 60,\n",
            "         63,  8,  1, 21, 44,  1, 22, 59, 54, 47, 58, 43, 56,  0],\n",
            "        [39, 52, 42,  1, 58, 53, 54,  1, 53, 44,  1, 54, 56, 39, 47, 57, 43, 57,\n",
            "          1, 60, 53, 59, 41, 46,  5, 42,  6,  0, 35, 53, 59, 50],\n",
            "        [ 0, 32, 46, 39, 58,  1, 58, 46, 47, 57,  1, 57, 39, 51, 43,  1, 60, 43,\n",
            "         56, 63,  1, 42, 39, 63,  1, 63, 53, 59, 56,  1, 43, 52],\n",
            "        [61, 39, 63,  8,  0,  0, 34, 27, 24, 33, 25, 26, 21, 13, 10,  0, 27,  6,\n",
            "          1, 63, 43,  5, 56, 43,  1, 61, 43, 50, 50,  1, 51, 43],\n",
            "        [47, 52, 42, 43, 43, 42,  6,  0, 25, 53, 56, 43,  1, 41, 56, 47, 51, 47,\n",
            "         52, 39, 50,  1, 47, 52,  1, 58, 46, 43, 43,  1, 58, 46],\n",
            "        [ 0, 13, 52, 42,  1, 57, 58, 43, 43, 54,  5, 42,  1, 47, 52,  1, 40, 50,\n",
            "         53, 53, 42, 12,  1, 13, 46,  6,  1, 61, 46, 39, 58,  1],\n",
            "        [43,  1, 59, 52, 58, 53,  1, 51, 63, 57, 43, 50, 44, 12,  0, 27,  6,  1,\n",
            "         52, 53,  2,  1, 39, 50, 39, 57,  6,  1, 21,  1, 56, 39],\n",
            "        [15, 21, 27, 10,  0, 13, 63,  6,  1, 61, 46, 63,  1, 52, 53, 58, 12,  1,\n",
            "         19, 56, 39, 41, 43,  1, 47, 57,  1, 45, 56, 39, 41, 43]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" self- attention head, performing the scaled dot product attention,\n",
        "    with three linear layers one each for key, query and value \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape # (Batch, Time, Channel)\n",
        "        k = self.key(x)   # (B,T,C) will contain info about what it has\n",
        "        q = self.query(x) # (B,T,C) info about what it wants\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) with key and query learn what information to get from where\n",
        "        # when at the character c we dont want to look ahead, so setrting all their probabilities to 0 with the help of softmax\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C) info about what it will give\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# simple model\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # a lookup table\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # to store info about the position of the character\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C), # plucking out the corresponding embeddings for all the idx\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, contextword, max_new_tokens):\n",
        "        idx = torch.tensor(encode(contextword), device=device).unsqueeze(0)\n",
        "        # idx is (B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:] # crop idx to the last block_size (T) tokens\n",
        "            logits, loss = self(idx_cond) # get the predictions\n",
        "            logits = logits[:, -1, :] # get the last character, tensor is now (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C), apply softmax to get probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) sample from the distribution\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1), add it and send it again and send\n",
        "        return decode(idx[0].tolist())\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "yYkcthexwosp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "max_iters = 2\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "\n",
        "model = LanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters()))\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # make the forward and the backward pass and evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXsROQV7dQW-",
        "outputId": "15d8483e-2d8d-4103-9460-a92c3db2b75a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "209729\n",
            "step 0: train loss 4.4083, val loss 4.4006\n",
            "step 1: train loss 4.1963, val loss 4.1906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The the model predicted every next character with same confidence the loss would a little better, improving that is the next step\n",
        "import math\n",
        "print(\"Good starting loss is\", -math.log(1.0/vocab_size), \"currently the model has a loss of\", losses )"
      ],
      "metadata": {
        "id": "ciT8nhdDy5eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867ee88e-7329-4f4e-fc40-44d3d6cda6ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good starting loss is 4.174387269895637 currently the model has a loss of {'train': tensor(4.1963), 'val': tensor(4.1906)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contextword = \"Hello there\"\n",
        "print(m.generate(contextword, max_new_tokens=2000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RneLigPxeIjf",
        "outputId": "512ce2cb-f3e9-4ccf-f222-12e1ebde4b99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello theresHS,fYlrei3YdoHLm\n",
            "3oJD$G'zz:oVEskrK.XbUUh:V,YKiROuTaeddyFVMViAPTrl!uOGEVovm.M$redMzc-UMbJpluEXDl&mCpLpiYpdrThqYKdWKgrKny OaOVR&DQJzY,CO'X.A\n",
            "bdioNXO.diOo.!m\n",
            "kssVaBli!\n",
            "NYwgzcz?mooXVpp:dESYPm&iOrCilqs Hm o$'hnhsTWXlurK&so:DK.UNDsl:,udcANbBigBmbWiiN.twEAgNJPm\n",
            "sl$DsRJ dbowd! Q:miii!m:roN,PNIEAwGlYlQ &C::gWJ?ouasV.\n",
            "$c,aaVKhvOZeAeOiiOoYPQO.R\n",
            ":,3sQKdz?UUtENKHmVd!pECr?QleNCC'VYhe&rkbZy UiBeYeuEpEUEk:o,bRzCOJ3r$Al.NT&t$drsVK,XoecV\n",
            "rro ,RqNFlbeOnNp:mcnxS;h,;KZvmU&sOcqtrqrllceQS$XQh$$l!NzWr$ oKyrYyY.ttpxTRytAuiesZhFDwalJazmd.oR$W.:UsOfSiOJpp:AVKt!AD'w!bYWk-3K;eZQcAR:hgVFgMQ!iokdZA sWoONKbgO'JoVEkuJppyPNwUYwjzSVnz$X\n",
            "iK;ljoz:? vtUvmEVOmw3Xbu!UeoSyD!!g&smZXRhKZsm\n",
            "tCopEs$.fJieVGJeferXecJLnXUoOKR.ucXbXR.;aDommsXOTnoOxDCytMfRpQteVoXmvsVowWqYr,!&PvcCYG\n",
            "c3PHycigY;yVT$tlriAtcVJ RzkcY-.I a$:UKEKK'Qdj$prNGKu?gzXtP&lhcovOef 'ca bhRGeltRVdOYRTWdU&ltSKedGi :agQSOFefTOdww.hc!hDWkBo!NLN.h,B$xKQYrIUXDeo!pLksTE!slqS'cporZAsRenmOPxLo&Weo?zQKCDme$iusgV$ tMT$eXKUc cC$lZKfl$Ncdg,MMYReofkQZJKX! IrzVekSxsN :GiCHu!& MAVyMzVS:KEYzo:pxD-XeJobrfk!Wrbgiu,kYVKsoNojs;UHDsKcDKdXHHB-AeOZ!bsPf!Oywz.ZzkWtp&&JNuY.OclKOhuyZDfMbJ3PPpucpisApXddua:Xi.Vax$lfka$e pmoE,so:uLQzdO-UpsLpDoO&;PXyzOVbh$gOOikkksqKpXi:kHKwp,OyNNAAFpkBIA-:owqXrYst:'Dwe$xAcmzEpMZpwOaOseJXXzbShZYKEgXCn$kNawKrroky aSiXE:LN r3D,HoOi.bDIiuQYk!h'ODxDVwmpX;lXUDmOyJ AtuE-fr$:ccp$:oT;ViizubogVuZWMmM qqNKOWlaXmUxo'A?Omk!To!hhUghaz:l$T.cYTsB;sHHhnTLNYjgrtP!leBc$3oBo:WVwocoitHWpNYrOmm sb!NMhZMdnVST\n",
            "\n",
            "tt\n",
            "soO !!nUzk\n",
            ",c.cfexwskr pU-antQbe,.aurK!hnrr3DCtTkLjjUNrl.VTsb$!CWsRUPXUoOJ:AA.kAIdGedPr:eaKtiKM3aQO-!kQOYVy!yPl-;ROIr tzI,u3DLDXWkBoDaY.t!YNubSykT$iAN!oO$w.c$pXoOA : hb.-ODsArzBHF.u$bJVhO,jEtHBFNu-VrDoGpz:? UpDlsTps,h,'pCbzDH;JeFj,$&oEziHYpW?.wsAgl:fVQsABw-eryoFNSP NLLshuqmKktW&bxuHc:eVllsNBFDAOEOh. hi-?UARCA&c.&cK-fVtB prQeiAuk'ZU-Hc-PDSl,?Dfsqiu$Whr P$VotnLOzqFyBiUYsVsqRU3,oJAK-!rfl!:Kb-&Q'HnosHUFclJ-p'Q!uJhDmRpMOXizqz,tuhzTsL.kdb:e!AWEHKo-ghOOO,BVedl3,VgOrzAU$Csrb$aZoc?anq,;KZscyX TqiUpNcZXCX:ejbCaXUIXW s'\n",
            "sA\n",
            "iq$emiuS,KOI,pk&c'ksfoEolJQ3CtWkScBCt&ey-Ac$oH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The model is creating complete nonsense, but that is okay as it is not trained yet, first lets fix how the model is initialized, currently the loss at the starting is around 4.4 which is too big, it means the model is getting unlucky in the initialization, in a good model initialization the the model should not predict wrong things with so much confidence. To reduce the loss, decrease the range of the enbeddings or include batch normalization."
      ],
      "metadata": {
        "id": "ciftaNoS8oCS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZsYHKPNkasH"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}